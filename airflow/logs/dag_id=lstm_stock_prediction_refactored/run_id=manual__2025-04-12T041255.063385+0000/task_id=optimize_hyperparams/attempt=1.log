[2025-04-12T04:13:02.187+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-12T04:13:02.202+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:12:55.063385+00:00 [queued]>
[2025-04-12T04:13:02.211+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:12:55.063385+00:00 [queued]>
[2025-04-12T04:13:02.212+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-12T04:13:02.222+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): optimize_hyperparams> on 2025-04-12 04:12:55.063385+00:00
[2025-04-12T04:13:02.227+0000] {standard_task_runner.py:72} INFO - Started process 709 to run task
[2025-04-12T04:13:02.229+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'lstm_stock_prediction_refactored', 'optimize_hyperparams', 'manual__2025-04-12T04:12:55.063385+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/lstm_refactored_dag.py', '--cfg-path', '/tmp/tmpo5eitvi6']
[2025-04-12T04:13:02.232+0000] {standard_task_runner.py:105} INFO - Job 81: Subtask optimize_hyperparams
[2025-04-12T04:13:02.268+0000] {task_command.py:467} INFO - Running <TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:12:55.063385+00:00 [running]> on host 38686bf18545
[2025-04-12T04:13:02.342+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***_team' AIRFLOW_CTX_DAG_ID='lstm_stock_prediction_refactored' AIRFLOW_CTX_TASK_ID='optimize_hyperparams' AIRFLOW_CTX_EXECUTION_DATE='2025-04-12T04:12:55.063385+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-12T04:12:55.063385+00:00'
[2025-04-12T04:13:02.343+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-12T04:13:02.353+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-04-12T04:13:02.355+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/src/models/optimize_hyperparams.py --config /opt/***/config/params.yaml']
[2025-04-12T04:13:02.364+0000] {subprocess.py:99} INFO - Output:
[2025-04-12T04:13:05.268+0000] {subprocess.py:106} INFO - mkdir -p failed for path /home/***/.cache/matplotlib: [Errno 13] Permission denied: '/home/***/.cache/matplotlib'
[2025-04-12T04:13:05.269+0000] {subprocess.py:106} INFO - Matplotlib created a temporary cache directory at /tmp/matplotlib-zeuwbj3a because there was an issue with the default path (/home/***/.cache/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
[2025-04-12T04:13:08.281+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:08,281] A new study created in memory with name: no-name-28f183af-a883-4cb6-8b18-84423eaad798
[2025-04-12T04:13:13.122+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:13,122] Trial 0 finished with value: 0.06542259640991688 and parameters: {'batch_size': 96, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0012000751830781198, 'model_type': 'lstm'}. Best is trial 0 with value: 0.06542259640991688.
[2025-04-12T04:13:15.531+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:15,530] Trial 1 finished with value: 0.07381762936711311 and parameters: {'batch_size': 112, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.002181160839806433, 'model_type': 'lstm_attention'}. Best is trial 0 with value: 0.06542259640991688.
[2025-04-12T04:13:18.592+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:18,591] Trial 2 finished with value: 0.10930225905030966 and parameters: {'batch_size': 48, 'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0016263253371696013, 'model_type': 'lstm'}. Best is trial 0 with value: 0.06542259640991688.
[2025-04-12T04:13:22.849+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:22,849] Trial 3 finished with value: 0.10127735137939453 and parameters: {'batch_size': 96, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.00907578896858958, 'model_type': 'lstm'}. Best is trial 0 with value: 0.06542259640991688.
[2025-04-12T04:13:28.640+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:28,640] Trial 4 finished with value: 0.07250350806862116 and parameters: {'batch_size': 48, 'hidden_size': 192, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.0007716464967987155, 'model_type': 'lstm'}. Best is trial 0 with value: 0.06542259640991688.
[2025-04-12T04:13:35.190+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:35,190] Trial 5 finished with value: 0.052161674946546555 and parameters: {'batch_size': 80, 'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.002568259019299351, 'model_type': 'lstm_cross_attention'}. Best is trial 5 with value: 0.052161674946546555.
[2025-04-12T04:13:37.538+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:37,538] Trial 6 finished with value: 0.10238214395940304 and parameters: {'batch_size': 48, 'hidden_size': 96, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.0001425148749326308, 'model_type': 'lstm'}. Best is trial 5 with value: 0.052161674946546555.
[2025-04-12T04:13:42.285+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:13:42,285] Trial 7 finished with value: 0.2932266294956207 and parameters: {'batch_size': 112, 'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00021226467690432945, 'model_type': 'lstm_cross_attention'}. Best is trial 5 with value: 0.052161674946546555.
[2025-04-12T04:14:35.617+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:14:35,617] Trial 8 finished with value: 0.04699878208339214 and parameters: {'batch_size': 128, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00027560691968241043, 'model_type': 'lstm_cross_attention'}. Best is trial 8 with value: 0.04699878208339214.
[2025-04-12T04:14:42.058+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:14:42,058] Trial 9 finished with value: 0.07795866578817368 and parameters: {'batch_size': 96, 'hidden_size': 192, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.0007117783884866137, 'model_type': 'lstm_attention'}. Best is trial 8 with value: 0.04699878208339214.
[2025-04-12T04:15:35.121+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:15:35,121] Trial 10 finished with value: 0.038122153282165526 and parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.00033271196813030907, 'model_type': 'lstm_cross_attention'}. Best is trial 10 with value: 0.038122153282165526.
[2025-04-12T04:16:13.009+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:16:13,009] Trial 11 finished with value: 0.039529388025403026 and parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.00032509975710396733, 'model_type': 'lstm_cross_attention'}. Best is trial 10 with value: 0.038122153282165526.
[2025-04-12T04:18:03.524+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:18:03,524] Trial 12 finished with value: 0.04559515938162804 and parameters: {'batch_size': 16, 'hidden_size': 224, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.00035622096804649694, 'model_type': 'lstm_cross_attention'}. Best is trial 10 with value: 0.038122153282165526.
[2025-04-12T04:19:41.635+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:19:41,635] Trial 13 finished with value: 0.020639446657150985 and parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00044478440549256443, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:20:44.270+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:20:44,270] Trial 14 finished with value: 0.040897012874484065 and parameters: {'batch_size': 32, 'hidden_size': 224, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0005580918104419217, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:21:39.306+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:21:39,306] Trial 15 finished with value: 0.04087147898972034 and parameters: {'batch_size': 32, 'hidden_size': 192, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00012299715885417825, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:22:19.047+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:22:19,047] Trial 16 finished with value: 0.05056002798179785 and parameters: {'batch_size': 64, 'hidden_size': 224, 'num_layers': 3, 'dropout_rate': 0.4, 'learning_rate': 0.0004978644781717965, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:22:23.380+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:22:23,380] Trial 17 finished with value: 0.0878205755725503 and parameters: {'batch_size': 16, 'hidden_size': 160, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.005304655412009991, 'model_type': 'lstm_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:23:44.443+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:23:44,443] Trial 18 finished with value: 0.041207855194807054 and parameters: {'batch_size': 32, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.00019402386365183105, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:23:57.494+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:23:57,494] Trial 19 finished with value: 0.047052618116140366 and parameters: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.001030706584665841, 'model_type': 'lstm_cross_attention'}. Best is trial 13 with value: 0.020639446657150985.
[2025-04-12T04:23:57.498+0000] {subprocess.py:106} INFO - Using device: cpu
[2025-04-12T04:23:57.499+0000] {subprocess.py:106} INFO - --- Loading Scaled Data from /opt/***/data/features/split_scaled_data.npz ---
[2025-04-12T04:23:57.500+0000] {subprocess.py:106} INFO - --- Finished Loading Scaled Data ---
[2025-04-12T04:23:57.500+0000] {subprocess.py:106} INFO - --- Loading Scalers from /opt/***/data/processed/scalers.pkl ---
[2025-04-12T04:23:57.501+0000] {subprocess.py:106} INFO - --- Finished Loading Scalers ---
[2025-04-12T04:23:57.501+0000] {subprocess.py:106} INFO - --- Starting Optuna Optimization (20 trials) ---
[2025-04-12T04:23:57.502+0000] {subprocess.py:106} INFO - Early stopping at epoch 18
[2025-04-12T04:23:57.502+0000] {subprocess.py:106} INFO - Early stopping at epoch 7
[2025-04-12T04:23:57.503+0000] {subprocess.py:106} INFO - Early stopping at epoch 15
[2025-04-12T04:23:57.503+0000] {subprocess.py:106} INFO - Early stopping at epoch 12
[2025-04-12T04:23:57.504+0000] {subprocess.py:106} INFO - Early stopping at epoch 7
[2025-04-12T04:23:57.504+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:23:57.506+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:23:57.507+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:23:57.507+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:23:57.508+0000] {subprocess.py:106} INFO - Early stopping at epoch 9
[2025-04-12T04:23:57.509+0000] {subprocess.py:106} INFO - Early stopping at epoch 6
[2025-04-12T04:23:57.509+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:23:57.510+0000] {subprocess.py:106} INFO - Early stopping at epoch 16
[2025-04-12T04:23:57.510+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:23:57.511+0000] {subprocess.py:106} INFO - Early stopping at epoch 15
[2025-04-12T04:23:57.511+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:23:57.512+0000] {subprocess.py:106} INFO - Early stopping at epoch 9
[2025-04-12T04:23:57.512+0000] {subprocess.py:106} INFO - Early stopping at epoch 12
[2025-04-12T04:23:57.513+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:23:57.513+0000] {subprocess.py:106} INFO - --- Finished Optuna Optimization ---
[2025-04-12T04:23:57.514+0000] {subprocess.py:106} INFO - Best trial value (loss): 0.020639446657150985
[2025-04-12T04:23:57.514+0000] {subprocess.py:106} INFO - Best parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00044478440549256443, 'model_type': 'lstm_cross_attention'}
[2025-04-12T04:23:57.515+0000] {subprocess.py:106} INFO - --- Saving Best Parameters to /opt/***/config/best_params.json ---
[2025-04-12T04:23:57.515+0000] {subprocess.py:106} INFO - --- Finished Saving Best Parameters ---
[2025-04-12T04:23:58.604+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-04-12T04:23:58.623+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-12T04:23:58.624+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=lstm_stock_prediction_refactored, task_id=optimize_hyperparams, run_id=manual__2025-04-12T04:12:55.063385+00:00, execution_date=20250412T041255, start_date=20250412T041302, end_date=20250412T042358
[2025-04-12T04:23:58.653+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-12T04:23:58.670+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-12T04:23:58.672+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
