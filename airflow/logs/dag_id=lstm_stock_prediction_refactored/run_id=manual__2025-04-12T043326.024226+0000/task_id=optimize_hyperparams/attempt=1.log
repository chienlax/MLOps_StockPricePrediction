[2025-04-12T04:33:32.005+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-12T04:33:32.020+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:33:26.024226+00:00 [queued]>
[2025-04-12T04:33:32.030+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:33:26.024226+00:00 [queued]>
[2025-04-12T04:33:32.032+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-04-12T04:33:32.047+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): optimize_hyperparams> on 2025-04-12 04:33:26.024226+00:00
[2025-04-12T04:33:32.056+0000] {standard_task_runner.py:72} INFO - Started process 3811 to run task
[2025-04-12T04:33:32.060+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'lstm_stock_prediction_refactored', 'optimize_hyperparams', 'manual__2025-04-12T04:33:26.024226+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/lstm_refactored_dag.py', '--cfg-path', '/tmp/tmp9b9ysq89']
[2025-04-12T04:33:32.063+0000] {standard_task_runner.py:105} INFO - Job 85: Subtask optimize_hyperparams
[2025-04-12T04:33:32.102+0000] {task_command.py:467} INFO - Running <TaskInstance: lstm_stock_prediction_refactored.optimize_hyperparams manual__2025-04-12T04:33:26.024226+00:00 [running]> on host 38686bf18545
[2025-04-12T04:33:32.177+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***_team' AIRFLOW_CTX_DAG_ID='lstm_stock_prediction_refactored' AIRFLOW_CTX_TASK_ID='optimize_hyperparams' AIRFLOW_CTX_EXECUTION_DATE='2025-04-12T04:33:26.024226+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-12T04:33:26.024226+00:00'
[2025-04-12T04:33:32.179+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-12T04:33:32.191+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-04-12T04:33:32.193+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/src/models/optimize_hyperparams.py --config /opt/***/config/params.yaml']
[2025-04-12T04:33:32.202+0000] {subprocess.py:99} INFO - Output:
[2025-04-12T04:33:34.003+0000] {subprocess.py:106} INFO - mkdir -p failed for path /home/***/.cache/matplotlib: [Errno 13] Permission denied: '/home/***/.cache/matplotlib'
[2025-04-12T04:33:34.005+0000] {subprocess.py:106} INFO - Matplotlib created a temporary cache directory at /tmp/matplotlib-76a13_fr because there was an issue with the default path (/home/***/.cache/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
[2025-04-12T04:33:36.435+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:36,435] A new study created in memory with name: no-name-03867da1-45b5-479e-9f20-58ad372522e5
[2025-04-12T04:33:39.606+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:39,606] Trial 0 finished with value: 0.05394297093153 and parameters: {'batch_size': 128, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.002092797325819911, 'model_type': 'lstm_attention'}. Best is trial 0 with value: 0.05394297093153.
[2025-04-12T04:33:44.652+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:44,652] Trial 1 finished with value: 0.044936709105968475 and parameters: {'batch_size': 48, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0005916438988725473, 'model_type': 'lstm'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:33:47.118+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:47,118] Trial 2 finished with value: 0.07282691262662411 and parameters: {'batch_size': 48, 'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00016399361887694814, 'model_type': 'lstm'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:33:53.946+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:53,946] Trial 3 finished with value: 0.06011483147740364 and parameters: {'batch_size': 16, 'hidden_size': 192, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.0003054918035878885, 'model_type': 'lstm'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:33:58.074+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:33:58,074] Trial 4 finished with value: 0.0734116980805993 and parameters: {'batch_size': 16, 'hidden_size': 160, 'num_layers': 2, 'dropout_rate': 0.1, 'learning_rate': 0.0008767450993419873, 'model_type': 'lstm_attention'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:34:06.751+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:34:06,751] Trial 5 finished with value: 0.07569218873977661 and parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.00013347253211817286, 'model_type': 'lstm_attention'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:34:08.488+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:34:08,487] Trial 6 finished with value: 0.05690261907875538 and parameters: {'batch_size': 16, 'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.002049773610075267, 'model_type': 'lstm'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:34:24.410+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:34:24,410] Trial 7 finished with value: 0.0879352626701196 and parameters: {'batch_size': 64, 'hidden_size': 96, 'num_layers': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0031831557362047526, 'model_type': 'lstm_cross_attention'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:34:31.727+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:34:31,727] Trial 8 finished with value: 0.07486027739942074 and parameters: {'batch_size': 16, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0012126133411684403, 'model_type': 'lstm_attention'}. Best is trial 1 with value: 0.044936709105968475.
[2025-04-12T04:34:57.751+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:34:57,751] Trial 9 finished with value: 0.04173847008496523 and parameters: {'batch_size': 112, 'hidden_size': 224, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.002226246823539324, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:35:13.188+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:35:13,187] Trial 10 finished with value: 0.07322005555033684 and parameters: {'batch_size': 128, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.009549657272246251, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:35:39.534+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:35:39,533] Trial 11 finished with value: 0.045302411541342735 and parameters: {'batch_size': 96, 'hidden_size': 224, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0005252079735781281, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:36:02.987+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:36:02,987] Trial 12 finished with value: 0.04593586828559637 and parameters: {'batch_size': 96, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.5, 'learning_rate': 0.003876195828044756, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:36:06.028+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:36:06,028] Trial 13 finished with value: 0.04635801864787936 and parameters: {'batch_size': 48, 'hidden_size': 224, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.00047232301689060976, 'model_type': 'lstm'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:37:00.255+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:37:00,254] Trial 14 finished with value: 0.04239165689796209 and parameters: {'batch_size': 96, 'hidden_size': 224, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0066579156273318275, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:37:30.805+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:37:30,804] Trial 15 finished with value: 0.24488015472888947 and parameters: {'batch_size': 96, 'hidden_size': 192, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.008226604865167169, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:37:45.108+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:37:45,108] Trial 16 finished with value: 0.10453145951032639 and parameters: {'batch_size': 112, 'hidden_size': 96, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0052101441532416875, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:38:11.464+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:38:11,464] Trial 17 finished with value: 0.045558568090200424 and parameters: {'batch_size': 80, 'hidden_size': 224, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.002018478949628458, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:38:33.119+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:38:33,119] Trial 18 finished with value: 0.08558381535112858 and parameters: {'batch_size': 112, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.005499273816337692, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:38:35.879+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:38:35,879] Trial 19 finished with value: 0.06021181587129831 and parameters: {'batch_size': 80, 'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00120956502746867, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:39:06.730+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:39:06,730] Trial 20 finished with value: 0.0603665579110384 and parameters: {'batch_size': 112, 'hidden_size': 224, 'num_layers': 3, 'dropout_rate': 0.2, 'learning_rate': 0.006355176065246871, 'model_type': 'lstm_cross_attention'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:39:10.984+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:39:10,984] Trial 21 finished with value: 0.04753408906981349 and parameters: {'batch_size': 48, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0007773656205587844, 'model_type': 'lstm'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:39:14.022+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:39:14,022] Trial 22 finished with value: 0.054620139921704926 and parameters: {'batch_size': 64, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00029310939935889936, 'model_type': 'lstm'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:39:17.156+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:39:17,156] Trial 23 finished with value: 0.055823832750320435 and parameters: {'batch_size': 32, 'hidden_size': 224, 'num_layers': 1, 'dropout_rate': 0.5, 'learning_rate': 0.0024884698322590283, 'model_type': 'lstm'}. Best is trial 9 with value: 0.04173847008496523.
[2025-04-12T04:39:40.681+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:39:40,681] Trial 24 finished with value: 0.036110974848270416 and parameters: {'batch_size': 80, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.001368678462713783, 'model_type': 'lstm_cross_attention'}. Best is trial 24 with value: 0.036110974848270416.
[2025-04-12T04:40:06.976+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:06,975] Trial 25 finished with value: 0.03941241465508938 and parameters: {'batch_size': 80, 'hidden_size': 224, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0014633068797581974, 'model_type': 'lstm_cross_attention'}. Best is trial 24 with value: 0.036110974848270416.
[2025-04-12T04:40:20.149+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:20,149] Trial 26 finished with value: 0.03405767772346735 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0013586615100618963, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:40:31.727+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:31,727] Trial 27 finished with value: 0.04236824344843626 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.0014079107728261608, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:40:34.651+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:34,651] Trial 28 finished with value: 0.05035550519824028 and parameters: {'batch_size': 64, 'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0015205239339207842, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:40:36.236+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:36,236] Trial 29 finished with value: 0.047328462824225426 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0008295632075243068, 'model_type': 'lstm_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:40:46.683+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:46,682] Trial 30 finished with value: 0.05806593969464302 and parameters: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0031886064889470513, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:40:57.756+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:40:57,756] Trial 31 finished with value: 0.04572908580303192 and parameters: {'batch_size': 96, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0017174647117995988, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:41:17.115+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:41:17,115] Trial 32 finished with value: 0.05448560696095228 and parameters: {'batch_size': 112, 'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0026139536052585803, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:41:30.380+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:41:30,380] Trial 33 finished with value: 0.053075509145855904 and parameters: {'batch_size': 128, 'hidden_size': 224, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0011297935747346314, 'model_type': 'lstm_cross_attention'}. Best is trial 26 with value: 0.03405767772346735.
[2025-04-12T04:41:46.237+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:41:46,237] Trial 34 finished with value: 0.03329696413129568 and parameters: {'batch_size': 80, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0006619708135281887, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:41:59.742+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:41:59,742] Trial 35 finished with value: 0.038603668101131916 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0006041649409104961, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:03.085+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:03,084] Trial 36 finished with value: 0.0750214916964372 and parameters: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.5, 'learning_rate': 0.00036719135977218817, 'model_type': 'lstm_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:14.134+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:14,134] Trial 37 finished with value: 0.03812533803284168 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0006440065982790094, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:19.274+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:19,274] Trial 38 finished with value: 0.040330272167921066 and parameters: {'batch_size': 80, 'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.5, 'learning_rate': 0.0007042302499885077, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:29.362+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:29,362] Trial 39 finished with value: 0.03903117589652538 and parameters: {'batch_size': 48, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0002448351824009413, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:31.782+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:31,782] Trial 40 finished with value: 0.0906970240175724 and parameters: {'batch_size': 96, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0009603207754067025, 'model_type': 'lstm_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:44.573+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:44,573] Trial 41 finished with value: 0.03895495366305113 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0006020762615123198, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:42:59.964+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:42:59,964] Trial 42 finished with value: 0.04704724190135797 and parameters: {'batch_size': 64, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.00020399899289547498, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:43:11.505+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:43:11,505] Trial 43 finished with value: 0.03939628694206476 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.5, 'learning_rate': 0.00040701638929935974, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:43:27.129+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:43:27,128] Trial 44 finished with value: 0.04454297758638859 and parameters: {'batch_size': 96, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0005903099571023437, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:43:34.780+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:43:34,780] Trial 45 finished with value: 0.04039972275495529 and parameters: {'batch_size': 80, 'hidden_size': 160, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0006802780683328117, 'model_type': 'lstm_cross_attention'}. Best is trial 34 with value: 0.03329696413129568.
[2025-04-12T04:43:55.068+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:43:55,067] Trial 46 finished with value: 0.030808929353952408 and parameters: {'batch_size': 64, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0009609766727669845, 'model_type': 'lstm_cross_attention'}. Best is trial 46 with value: 0.030808929353952408.
[2025-04-12T04:43:57.306+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:43:57,305] Trial 47 finished with value: 0.06678479630500078 and parameters: {'batch_size': 48, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0009778928679614397, 'model_type': 'lstm_attention'}. Best is trial 46 with value: 0.030808929353952408.
[2025-04-12T04:44:08.074+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:44:08,074] Trial 48 finished with value: 0.04910487805803617 and parameters: {'batch_size': 64, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0004691734694556669, 'model_type': 'lstm_cross_attention'}. Best is trial 46 with value: 0.030808929353952408.
[2025-04-12T04:44:13.349+0000] {subprocess.py:106} INFO - [I 2025-04-12 04:44:13,349] Trial 49 finished with value: 0.04579531401395798 and parameters: {'batch_size': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0011116272487257512, 'model_type': 'lstm_cross_attention'}. Best is trial 46 with value: 0.030808929353952408.
[2025-04-12T04:44:13.353+0000] {subprocess.py:106} INFO - Using device: cpu
[2025-04-12T04:44:13.354+0000] {subprocess.py:106} INFO - --- Loading Scaled Data from /opt/***/data/features/split_scaled_data.npz ---
[2025-04-12T04:44:13.355+0000] {subprocess.py:106} INFO - --- Finished Loading Scaled Data ---
[2025-04-12T04:44:13.355+0000] {subprocess.py:106} INFO - --- Loading Scalers from /opt/***/data/processed/scalers.pkl ---
[2025-04-12T04:44:13.356+0000] {subprocess.py:106} INFO - --- Finished Loading Scalers ---
[2025-04-12T04:44:13.357+0000] {subprocess.py:106} INFO - --- Starting Optuna Optimization (50 trials) ---
[2025-04-12T04:44:13.357+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:44:13.358+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:44:13.358+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:44:13.359+0000] {subprocess.py:106} INFO - Early stopping at epoch 7
[2025-04-12T04:44:13.359+0000] {subprocess.py:106} INFO - Early stopping at epoch 9
[2025-04-12T04:44:13.360+0000] {subprocess.py:106} INFO - Early stopping at epoch 10
[2025-04-12T04:44:13.360+0000] {subprocess.py:106} INFO - Early stopping at epoch 5
[2025-04-12T04:44:13.361+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:44:13.361+0000] {subprocess.py:106} INFO - Early stopping at epoch 19
[2025-04-12T04:44:13.362+0000] {subprocess.py:106} INFO - Early stopping at epoch 15
[2025-04-12T04:44:13.362+0000] {subprocess.py:106} INFO - Early stopping at epoch 19
[2025-04-12T04:44:13.363+0000] {subprocess.py:106} INFO - Early stopping at epoch 19
[2025-04-12T04:44:13.363+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:44:13.363+0000] {subprocess.py:106} INFO - Early stopping at epoch 12
[2025-04-12T04:44:13.364+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:44:13.364+0000] {subprocess.py:106} INFO - Early stopping at epoch 10
[2025-04-12T04:44:13.365+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:44:13.365+0000] {subprocess.py:106} INFO - Early stopping at epoch 6
[2025-04-12T04:44:13.366+0000] {subprocess.py:106} INFO - Early stopping at epoch 15
[2025-04-12T04:44:13.366+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:44:13.366+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:44:13.367+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:44:13.367+0000] {subprocess.py:106} INFO - Early stopping at epoch 9
[2025-04-12T04:44:13.368+0000] {subprocess.py:106} INFO - Early stopping at epoch 12
[2025-04-12T04:44:13.368+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:44:13.369+0000] {subprocess.py:106} INFO - Early stopping at epoch 10
[2025-04-12T04:44:13.369+0000] {subprocess.py:106} INFO - Early stopping at epoch 15
[2025-04-12T04:44:13.370+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:44:13.371+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:44:13.371+0000] {subprocess.py:106} INFO - Early stopping at epoch 16
[2025-04-12T04:44:13.372+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:44:13.373+0000] {subprocess.py:106} INFO - Early stopping at epoch 16
[2025-04-12T04:44:13.374+0000] {subprocess.py:106} INFO - Early stopping at epoch 12
[2025-04-12T04:44:13.374+0000] {subprocess.py:106} INFO - Early stopping at epoch 9
[2025-04-12T04:44:13.375+0000] {subprocess.py:106} INFO - Early stopping at epoch 13
[2025-04-12T04:44:13.376+0000] {subprocess.py:106} INFO - Early stopping at epoch 14
[2025-04-12T04:44:13.376+0000] {subprocess.py:106} INFO - Early stopping at epoch 19
[2025-04-12T04:44:13.377+0000] {subprocess.py:106} INFO - Early stopping at epoch 17
[2025-04-12T04:44:13.378+0000] {subprocess.py:106} INFO - Early stopping at epoch 16
[2025-04-12T04:44:13.378+0000] {subprocess.py:106} INFO - Early stopping at epoch 10
[2025-04-12T04:44:13.378+0000] {subprocess.py:106} INFO - Early stopping at epoch 6
[2025-04-12T04:44:13.379+0000] {subprocess.py:106} INFO - Early stopping at epoch 11
[2025-04-12T04:44:13.379+0000] {subprocess.py:106} INFO - Early stopping at epoch 8
[2025-04-12T04:44:13.380+0000] {subprocess.py:106} INFO - --- Finished Optuna Optimization ---
[2025-04-12T04:44:13.380+0000] {subprocess.py:106} INFO - Best trial value (loss): 0.030808929353952408
[2025-04-12T04:44:13.380+0000] {subprocess.py:106} INFO - Best parameters: {'batch_size': 64, 'hidden_size': 192, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0009609766727669845, 'model_type': 'lstm_cross_attention'}
[2025-04-12T04:44:13.381+0000] {subprocess.py:106} INFO - --- Saving Best Parameters to /opt/***/config/best_params.json ---
[2025-04-12T04:44:13.381+0000] {subprocess.py:106} INFO - --- Finished Saving Best Parameters ---
[2025-04-12T04:44:14.475+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-04-12T04:44:14.496+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-12T04:44:14.497+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=lstm_stock_prediction_refactored, task_id=optimize_hyperparams, run_id=manual__2025-04-12T04:33:26.024226+00:00, execution_date=20250412T043326, start_date=20250412T043332, end_date=20250412T044414
[2025-04-12T04:44:14.541+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-12T04:44:14.560+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-12T04:44:14.561+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
