data_loading:
  tickers: ['AAPL', 'MA', 'CSCO', 'MSFT', 'AMZN', 'GOOG', 'IBM']
  period: "3y"
  interval: "1d"
  fetch_delay: 5

feature_engineering:
  correlation_threshold: 0.9
  sequence_length: 30
  prediction_length: 1 # Currently only supporting 1-day ahead

optimization:
  n_trials: 1
  epochs: 5 
  patience: 5

training:
  epochs: 5

database:
  dbname: 'stock_prediction'
  user: 'admin' #Not remember
  password: 'admin'
  host: 'localhost'
  port: '5432'

monitoring:
  performance_thresholds:
    mape_max: 0.10        # Threshold: Max 7% MAPE
    direction_accuracy_min: 0.55 # Threshold: Min 53% directional accuracy
  evaluation_lag_days: 1 # How many days to wait for actuals (e.g., T+1 for stock prices)

airflow_dags:
  daily_operations_dag_id: "daily_stock_operations"
  retraining_pipeline_dag_id: "stock_model_retraining_pipeline"


output_paths:
  # Use ABSOLUTE paths inside the container that point to the mounted volume - if you want to use relative paths, you need to mount the volume accordingly. If not, the data crawled will not appear in your local directory.
  raw_data_template: "/opt/airflow/data/raw/{ticker}_raw.pkl"
  # processed_data_path: "/opt/airflow/data/processed/all_processed_data.npz" # Saving as numpy archive
  # scalers_path: "/opt/airflow/data/processed/scalers.pkl"
  # split_data_path: "/opt/airflow/data/features/split_scaled_data.npz"
  # best_params_path: "/opt/airflow/config/best_params.json" # Config might also need absolute if written by a task
  # Note: We are mounting ./config to /opt/airflow/config, so this path for best_params is correct

mlflow:
  experiment_name: "Stock_Price_Prediction_LSTM_Refactored"
  final_run_name: "final_model_training"
  optuna_run_prefix: "optuna_trial_" # Prefix for Optuna runs
  mlflow_uri: "http://mlflow-server:5000" # MLflow server URI (internal to Docker network)
  tracking_uri: "http://mlflow-server:5000" # MLflow tracking URI (internal to Docker network)