{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.14.5\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# %% [markdown]\n",
    "# # Debugging Notebook: make_dataset.py File Saving\n",
    "#\n",
    "# This notebook replicates the logic from `src/data/make_dataset.py` to interactively debug why the output file (`all_processed_data.npz`) might not be appearing on the host machine despite logs indicating successful saving.\n",
    "#\n",
    "# **Instructions:**\n",
    "# 1. Run this notebook using a kernel connected to your Docker environment (e.g., by running `jupyter lab` inside the `airflow-scheduler` container) or a local environment with identical dependencies.\n",
    "# 2. Ensure the `config_path` variable below points to the correct location of your `params.yaml` *within the execution environment*.\n",
    "# 3. Run cells sequentially (`Shift+Enter`).\n",
    "# 4. Observe log output and the final file existence check.\n",
    "\n",
    "# %%\n",
    "# Core Imports\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "import os # For final file check\n",
    "\n",
    "# %%\n",
    "# --- Configure logging ---\n",
    "# Configure logging to output to the notebook's console output\n",
    "logger = logging.getLogger(\"make_dataset_debug\")\n",
    "logger.setLevel(logging.INFO) # Set to DEBUG for more verbose output if needed\n",
    "\n",
    "# Check if handlers are already added (important in interactive environments)\n",
    "if not logger.hasHandlers():\n",
    "    handler = logging.StreamHandler(stream=sys.stdout) # Explicitly use stdout\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False # Prevent duplicate logs if root logger is configured\n",
    "\n",
    "logger.info(\"Logging configured for notebook.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Function Definitions\n",
    "# Copy all function definitions from `src/data/make_dataset.py` here.\n",
    "\n",
    "# %%\n",
    "def load_data(tickers_list, period, interval, fetch_delay, output_dir):\n",
    "    \"\"\"Loads data for tickers and saves each as a pickle file.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Saving raw data to: {output_dir}\")\n",
    "\n",
    "    for t in tickers_list:\n",
    "        output_path = output_dir / f\"{t}_raw.pkl\"\n",
    "        # Re-downloading for debugging simplicity in notebook, remove 'if' to force download\n",
    "        # if output_path.exists():\n",
    "        #     logger.info(f\"Skipping download for {t}, file exists: {output_path}\")\n",
    "        #     continue\n",
    "        try:\n",
    "            logger.info(f\"Attempting download for {t}...\")\n",
    "            data = yf.Ticker(t).history(period=period, interval=interval)\n",
    "            if not data.empty:\n",
    "                with open(output_path, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                logger.info(f\"Loaded and saved raw data for {t} to {output_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"No data loaded for {t}\")\n",
    "            time.sleep(fetch_delay)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {t}: {e}\", exc_info=True)\n",
    "\n",
    "# %%\n",
    "def add_technical_indicators(ticker_data):\n",
    "    logger.info(\"Adding technical indicators...\")\n",
    "    for t in ticker_data:\n",
    "        df = ticker_data[t]\n",
    "        if df is None or df.empty:\n",
    "            logger.warning(f\"Skipping indicators for {t} due to empty data.\")\n",
    "            continue\n",
    "        logger.debug(f\"Processing indicators for {t}...\")\n",
    "        # Wrap potentially problematic indicators in try-except if needed\n",
    "        try:\n",
    "            # Core indicators\n",
    "            df['EMA_50'] = ta.trend.EMAIndicator(df['Close'], 50).ema_indicator()\n",
    "            df['EMA_200'] = ta.trend.EMAIndicator(df['Close'], 200).ema_indicator()\n",
    "            df['RSI'] = ta.momentum.RSIIndicator(df['Close'], 14).rsi()\n",
    "            df['MACD'] = ta.trend.MACD(df['Close'], window_fast=12, window_sign=9, window_slow=26).macd()\n",
    "            df['BB_High'] = ta.volatility.BollingerBands(df['Close'], window=15, window_dev=2).bollinger_hband()\n",
    "            df['BB_Low'] = ta.volatility.BollingerBands(df['Close'], window=15, window_dev=2).bollinger_lband()\n",
    "            df['ATR'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close'], window=14).average_true_range()\n",
    "            df['OBV'] = ta.volume.OnBalanceVolumeIndicator(df['Close'], df['Volume']).on_balance_volume()\n",
    "            df['MFI'] = ta.volume.MFIIndicator(df['High'], df['Low'], df['Close'], df['Volume'], window=14).money_flow_index()\n",
    "            df['ADX'] = ta.trend.ADXIndicator(df['High'], df['Low'], df['Close'], window=14).adx()\n",
    "            # Add other indicators from your script...\n",
    "            # Additional trend indicators\n",
    "            df['SMA_50'] = ta.trend.SMAIndicator(df['Close'], 50).sma_indicator()\n",
    "            df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(df['High'], df['Low'], df['Close'], df['Volume']).volume_weighted_average_price()\n",
    "            df['PSAR'] = ta.trend.PSARIndicator(df['High'], df['Low'], df['Close']).psar()\n",
    "            # Momentum indicators\n",
    "            df['Stochastic_K'] = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close']).stoch()\n",
    "            df['Stochastic_D'] = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close']).stoch_signal()\n",
    "            df['CCI'] = ta.trend.CCIIndicator(df['High'], df['Low'], df['Close']).cci()\n",
    "            df['Williams_R'] = ta.momentum.WilliamsRIndicator(df['High'], df['Low'], df['Close']).williams_r()\n",
    "            # Volatility indicators\n",
    "            df['Donchian_High'] = ta.volatility.DonchianChannel(df['High'], df['Low'], df['Close'], window=20).donchian_channel_hband()\n",
    "            df['Donchian_Low'] = ta.volatility.DonchianChannel(df['High'], df['Low'], df['Close'], window=20).donchian_channel_lband()\n",
    "            df['Keltner_High'] = ta.volatility.KeltnerChannel(df['High'], df['Low'], df['Close']).keltner_channel_hband()\n",
    "            df['Keltner_Low'] = ta.volatility.KeltnerChannel(df['High'], df['Low'], df['Close']).keltner_channel_lband()\n",
    "            # Price-based features\n",
    "            df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            df['Price_Rate_Of_Change'] = ta.momentum.ROCIndicator(df['Close'], 12).roc()\n",
    "            # Volume-based indicators\n",
    "            df['Volume_SMA'] = ta.trend.SMAIndicator(df['Volume'], 20).sma_indicator()\n",
    "            df['Chaikin_Money_Flow'] = ta.volume.ChaikinMoneyFlowIndicator(df['High'], df['Low'], df['Close'], df['Volume'], window=20).chaikin_money_flow()\n",
    "            df['Force_Index'] = ta.volume.ForceIndexIndicator(df['Close'], df['Volume']).force_index()\n",
    "            # Trend-strength indicators\n",
    "            df['DI_Positive'] = ta.trend.ADXIndicator(df['High'], df['Low'], df['Close'], window=14).adx_pos()\n",
    "            df['DI_Negative'] = ta.trend.ADXIndicator(df['High'], df['Low'], df['Close'], window=14).adx_neg()\n",
    "\n",
    "            ticker_data[t] = df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding indicators for {t}: {e}\", exc_info=True)\n",
    "            # Optionally set df to None or keep partial data\n",
    "            # ticker_data[t] = df # Keep partially processed data\n",
    "            pass # Continue with next ticker\n",
    "\n",
    "    logger.info(\"Finished adding technical indicators.\")\n",
    "    return ticker_data\n",
    "\n",
    "# %%\n",
    "def preprocess_data(df):\n",
    "    logger.debug(\"Preprocessing data: Filling NaNs and creating Target.\")\n",
    "    # Fill forward then backward to handle NaNs\n",
    "    df_filled = df.ffill().bfill() # Use different variable name to avoid modifying original df directly if needed later\n",
    "    # Create target variable (next day's close price)\n",
    "    df_filled['Target'] = df_filled['Close'].shift(-1)\n",
    "    # Drop the last row since it will have NaN target\n",
    "    df_processed = df_filled.dropna()\n",
    "    logger.debug(f\"Original shape: {df.shape}, Processed shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "# %%\n",
    "def align_and_process_data(ticker_data):\n",
    "    logger.info(\"Aligning and processing data across all tickers.\")\n",
    "    tickers = list(ticker_data.keys())\n",
    "    logger.debug(f\"Processing tickers: {tickers}\")\n",
    "\n",
    "    # Preprocess each dataframe first\n",
    "    processed_ticker_data = {}\n",
    "    for t in tickers:\n",
    "         if ticker_data[t] is not None and not ticker_data[t].empty:\n",
    "              processed_ticker_data[t] = preprocess_data(ticker_data[t].copy()) # Process a copy\n",
    "         else:\n",
    "              logger.warning(f\"Skipping preprocessing for {t} due to empty or None data.\")\n",
    "\n",
    "    # Filter out any potentially empty dataframes after preprocessing\n",
    "    processed_ticker_data = {t: df for t, df in processed_ticker_data.items() if not df.empty}\n",
    "    if not processed_ticker_data:\n",
    "        logger.error(\"No valid data remaining after preprocessing step.\")\n",
    "        return None, None, None, None # Return Nones to indicate failure\n",
    "\n",
    "    # Get final list of tickers and common indices\n",
    "    final_tickers = list(processed_ticker_data.keys())\n",
    "    logger.info(f\"Tickers remaining after preprocessing: {final_tickers}\")\n",
    "    try:\n",
    "        all_indices = pd.concat(processed_ticker_data.values()).index.unique()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting common indices: {e}\", exc_info=True)\n",
    "        # Trying alternative if concat fails (e.g., only one ticker left)\n",
    "        if len(final_tickers) == 1:\n",
    "             all_indices = processed_ticker_data[final_tickers[0]].index\n",
    "        else: # If multiple tickers but concat failed, re-raise or handle\n",
    "             raise ValueError(\"Could not determine common indices.\") from e\n",
    "\n",
    "    logger.debug(f\"Total common timesteps: {len(all_indices)}\")\n",
    "\n",
    "    # Reindex and align\n",
    "    aligned_data = {}\n",
    "    for t in final_tickers:\n",
    "        aligned_data[t] = processed_ticker_data[t].reindex(index=all_indices).sort_index()\n",
    "\n",
    "    # Get feature columns (excluding Target) from the first *valid* aligned dataframe\n",
    "    first_ticker = final_tickers[0]\n",
    "    feature_columns = [col for col in aligned_data[first_ticker].columns if col != 'Target']\n",
    "    num_features = len(feature_columns)\n",
    "    num_stocks = len(final_tickers)\n",
    "    logger.info(f\"Number of stocks: {num_stocks}, Number of features: {num_features}\")\n",
    "\n",
    "    # Create 3D arrays: (timesteps, stocks, features)\n",
    "    processed_data_np = np.zeros((len(all_indices), num_stocks, num_features))\n",
    "    targets_np = np.zeros((len(all_indices), num_stocks))\n",
    "\n",
    "    for i, ticker in enumerate(final_tickers):\n",
    "        # Fill NaNs that might have been introduced by reindexing before getting values\n",
    "        df_aligned_filled = aligned_data[ticker][feature_columns + ['Target']].ffill().bfill()\n",
    "\n",
    "        # Check if df_aligned_filled still has NaNs after ffill/bfill (shouldn't happen often)\n",
    "        if df_aligned_filled.isnull().values.any():\n",
    "            logger.warning(f\"NaNs still present in aligned data for {ticker} after ffill/bfill. Check source data.\")\n",
    "            # Handle as needed: maybe fill with 0, mean, or raise error\n",
    "            df_aligned_filled = df_aligned_filled.fillna(0) # Example: fill remaining with 0\n",
    "\n",
    "        try:\n",
    "             processed_data_np[:, i, :] = df_aligned_filled[feature_columns].values\n",
    "             targets_np[:, i] = df_aligned_filled['Target'].values\n",
    "        except ValueError as ve:\n",
    "             logger.error(f\"Shape mismatch error processing {ticker}. Aligned shape: {df_aligned_filled.shape}, Expected features: {num_features}\", exc_info=True)\n",
    "             raise ve # Re-raise error\n",
    "\n",
    "    # Clean any remaining NaNs across the entire arrays (e.g., if a whole timestep was NaN)\n",
    "    nan_mask_data = np.isnan(processed_data_np).any(axis=(1, 2))\n",
    "    nan_mask_targets = np.isnan(targets_np).any(axis=1)\n",
    "    combined_nan_mask = nan_mask_data | nan_mask_targets\n",
    "\n",
    "    if np.all(combined_nan_mask):\n",
    "         logger.error(\"All timesteps contain NaNs after alignment. Cannot proceed.\")\n",
    "         return None, None, None, None\n",
    "\n",
    "    processed_data_final = processed_data_np[~combined_nan_mask]\n",
    "    targets_final = targets_np[~combined_nan_mask]\n",
    "    logger.info(f\"Final processed data shape after NaN cleaning: {processed_data_final.shape}\")\n",
    "    logger.info(f\"Final targets shape after NaN cleaning: {targets_final.shape}\")\n",
    "\n",
    "    if processed_data_final.shape[0] == 0:\n",
    "         logger.error(\"No data left after NaN cleaning. Check input data quality and alignment.\")\n",
    "         return None, None, None, None\n",
    "\n",
    "    return processed_data_final, targets_final, feature_columns, final_tickers\n",
    "\n",
    "\n",
    "# %%\n",
    "def filter_correlated_features(ticker_data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Analyze feature correlations and remove highly correlated features\n",
    "    Returns filtered data and list of features to keep\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering highly correlated features with threshold {threshold}\")\n",
    "\n",
    "    if not ticker_data:\n",
    "         logger.warning(\"Ticker data is empty, skipping correlation filtering.\")\n",
    "         return {}, []\n",
    "\n",
    "    # Use the first ticker as reference for correlation analysis\n",
    "    first_ticker = list(ticker_data.keys())[0]\n",
    "    df = ticker_data[first_ticker].copy()\n",
    "\n",
    "    # Ensure only numeric columns are used for correlation\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        logger.warning(f\"No numeric columns found for correlation analysis in ticker {first_ticker}.\")\n",
    "        return ticker_data, df.columns.tolist() # Return original if no numeric cols\n",
    "\n",
    "    logger.debug(f\"Calculating correlation matrix on {len(numeric_cols)} numeric columns.\")\n",
    "    corr_matrix = df[numeric_cols].corr().abs()\n",
    "\n",
    "    # Create a mask for the upper triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    logger.info(f\"Identified {len(to_drop)} potentially highly correlated numeric features to remove: {to_drop}\")\n",
    "\n",
    "    if 'Close' in to_drop:\n",
    "        logger.info(\"Excluding 'Close' column from removal list.\")\n",
    "        to_drop.remove('Close')\n",
    "\n",
    "    logger.info(f\"Final list of {len(to_drop)} numeric features to remove: {to_drop}\")\n",
    "\n",
    "    # Determine features to keep (all original columns minus the ones to drop)\n",
    "    # This preserves non-numeric columns if any existed\n",
    "    features_to_keep = [col for col in df.columns if col not in to_drop]\n",
    "    logger.info(f\"Total features to keep: {len(features_to_keep)}\")\n",
    "\n",
    "    # Filter features for all tickers\n",
    "    filtered_ticker_data = {}\n",
    "    for ticker, data in ticker_data.items():\n",
    "        if data is not None:\n",
    "             # Select only the columns that exist in this specific dataframe\n",
    "             cols_to_select = [col for col in features_to_keep if col in data.columns]\n",
    "             filtered_ticker_data[ticker] = data[cols_to_select]\n",
    "        else:\n",
    "             filtered_ticker_data[ticker] = None\n",
    "\n",
    "\n",
    "    return filtered_ticker_data, features_to_keep\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Configuration Setup\n",
    "# Define the path to the `params.yaml` file. **Ensure this path is correct for the environment where you are running the notebook.** If running inside the Docker container as recommended, `/opt/airflow/config/params.yaml` should be correct.\n",
    "\n",
    "# %%\n",
    "# CONFIGURATION\n",
    "# Adjust this path if necessary!\n",
    "config_path = '/opt/airflow/config/params.yaml' # Path inside the container\n",
    "\n",
    "logger.info(f\"Using configuration file: {config_path}\")\n",
    "\n",
    "# Verify config file exists\n",
    "if not Path(config_path).is_file():\n",
    "    logger.error(f\"Configuration file not found at: {config_path}\")\n",
    "    # Raise error or handle appropriately\n",
    "    raise FileNotFoundError(f\"Configuration file not found at: {config_path}\")\n",
    "else:\n",
    "    logger.info(\"Configuration file found.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Main Processing Logic (Adapted from `run_processing`)\n",
    "# This cell executes the core data loading and processing steps.\n",
    "\n",
    "# %%\n",
    "# MAIN EXECUTION\n",
    "try:\n",
    "    with open(config_path, 'r') as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    logger.info(f\"Loaded parameters: {params}\")\n",
    "\n",
    "    # Define paths from config - Use Path objects for robustness\n",
    "    raw_data_dir = Path(params['output_paths']['raw_data_template']).parent\n",
    "    processed_output_path = Path(params['output_paths']['processed_data_path'])\n",
    "    logger.info(f\"Raw data directory: {raw_data_dir}\")\n",
    "    logger.info(f\"Processed data output path: {processed_output_path}\")\n",
    "\n",
    "    # Create parent directory for output if it doesn't exist\n",
    "    processed_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Ensured output directory exists: {processed_output_path.parent}\")\n",
    "\n",
    "\n",
    "    tickers_list = params['data_loading']['tickers']\n",
    "    period = params['data_loading']['period']\n",
    "    interval = params['data_loading']['interval']\n",
    "    fetch_delay = params['data_loading']['fetch_delay']\n",
    "    corr_threshold = params['feature_engineering']['correlation_threshold']\n",
    "\n",
    "    # 1. Load Raw Data (or ensure it's downloaded)\n",
    "    logger.info(\"--- Step 1: Starting Data Loading ---\")\n",
    "    load_data(tickers_list, period, interval, fetch_delay, raw_data_dir)\n",
    "    logger.info(\"--- Step 1: Finished Data Loading ---\")\n",
    "\n",
    "    # Load from saved pickles\n",
    "    logger.info(\"--- Loading raw data from pickle files ---\")\n",
    "    ticker_data = {}\n",
    "    for t in tickers_list:\n",
    "        raw_path = raw_data_dir / f\"{t}_raw.pkl\"\n",
    "        if raw_path.exists():\n",
    "            try:\n",
    "                with open(raw_path, 'rb') as f:\n",
    "                    ticker_data[t] = pickle.load(f)\n",
    "                logger.info(f\"Loaded pickle for {t}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading pickle file for {t} from {raw_path}\", exc_info=True)\n",
    "                ticker_data[t] = None # Mark as None if loading failed\n",
    "        else:\n",
    "            logger.warning(f\"Raw data pickle file not found for {t} at {raw_path}\")\n",
    "            ticker_data[t] = None # Mark as None if file not found\n",
    "\n",
    "    # Filter out any tickers where data wasn't loaded or failed to load\n",
    "    ticker_data = {k: v for k, v in ticker_data.items() if v is not None and not v.empty}\n",
    "    if not ticker_data:\n",
    "        raise ValueError(\"No valid ticker data could be loaded from pickle files. Exiting.\")\n",
    "    loaded_tickers = list(ticker_data.keys())\n",
    "    logger.info(f\"Successfully loaded data for tickers: {loaded_tickers}\")\n",
    "\n",
    "\n",
    "    # 2. Add Technical Indicators\n",
    "    logger.info(\"--- Step 2: Starting Feature Engineering (Indicators) ---\")\n",
    "    ticker_data = add_technical_indicators(ticker_data)\n",
    "    logger.info(\"--- Step 2: Finished Feature Engineering (Indicators) ---\")\n",
    "\n",
    "    # 3. Filter Correlated Features\n",
    "    logger.info(\"--- Step 3: Starting Feature Filtering ---\")\n",
    "    ticker_data, remaining_features = filter_correlated_features(ticker_data, corr_threshold)\n",
    "    logger.info(f\"Features remaining after filtering: {len(remaining_features)}\")\n",
    "    logger.info(\"--- Step 3: Finished Feature Filtering ---\")\n",
    "\n",
    "    # 4. Align and Process\n",
    "    logger.info(\"--- Step 4: Starting Data Alignment & Processing ---\")\n",
    "    processed_data, targets, feature_columns, final_tickers = align_and_process_data(ticker_data)\n",
    "    if processed_data is None:\n",
    "         raise ValueError(\"Data alignment and processing failed. Check logs.\")\n",
    "    logger.info(f\"Processed data shape: {processed_data.shape}\")\n",
    "    logger.info(f\"Targets shape: {targets.shape}\")\n",
    "    logger.info(f\"Final tickers in order: {final_tickers}\")\n",
    "    logger.info(\"--- Step 4: Finished Data Alignment & Processing ---\")\n",
    "\n",
    "    # 5. Save Processed Data\n",
    "    logger.info(f\"--- Step 5: Saving Processed Data to {processed_output_path} ---\")\n",
    "    absolute_save_path = processed_output_path.resolve()\n",
    "    logger.info(f\"Attempting to save to absolute path: {absolute_save_path}\")\n",
    "    try:\n",
    "        np.savez(\n",
    "            processed_output_path,\n",
    "            processed_data=processed_data,\n",
    "            targets=targets,\n",
    "            feature_columns=np.array(feature_columns, dtype=object),\n",
    "            tickers=np.array(final_tickers, dtype=object)\n",
    "        )\n",
    "        logger.info(f\"np.savez command executed for path: {processed_output_path}\")\n",
    "\n",
    "        # Add an existence check right after saving\n",
    "        if absolute_save_path.exists():\n",
    "            logger.info(f\"Verified file exists immediately after saving at: {absolute_save_path}\")\n",
    "            file_size = absolute_save_path.stat().st_size\n",
    "            logger.info(f\"File size: {file_size} bytes\")\n",
    "            if file_size == 0:\n",
    "                logger.warning(\"File was created but has ZERO size. Saving might have failed silently or data was empty.\")\n",
    "        else:\n",
    "            logger.error(f\"!!! File DOES NOT exist immediately after saving attempt at: {absolute_save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save processed data to {processed_output_path}\", exc_info=True)\n",
    "        raise # Re-raise the exception\n",
    "\n",
    "    logger.info(\"--- Step 5: Finished Saving Processed Data ---\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Log specific message handled above, just pass here or log completion status\n",
    "    logger.critical(\"Process failed due to missing configuration file.\")\n",
    "except ValueError as ve:\n",
    "    logger.critical(f\"Process failed due to a ValueError: {ve}\", exc_info=True)\n",
    "except Exception as e:\n",
    "    logger.critical(\"An unexpected error occurred during the data processing pipeline.\", exc_info=True)\n",
    "\n",
    "logger.info(\"--- Processing finished (check logs for errors) ---\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Final File Check\n",
    "# Let's explicitly check if the file exists at the target path *after* the main logic has run.\n",
    "\n",
    "# %%\n",
    "# FINAL CHECK\n",
    "logger.info(\"--- Performing final check for output file ---\")\n",
    "\n",
    "# Define the expected output path again (ensure it matches the one used above)\n",
    "config_path_check = '/opt/airflow/config/params.yaml' # Use the same path as defined earlier\n",
    "with open(config_path_check, 'r') as f:\n",
    "    params_check = yaml.safe_load(f)\n",
    "processed_output_path_check = Path(params_check['output_paths']['processed_data_path'])\n",
    "absolute_path_check = processed_output_path_check.resolve() # Get absolute path *inside* the environment\n",
    "\n",
    "logger.info(f\"Checking for file at absolute path: {absolute_path_check}\")\n",
    "\n",
    "if absolute_path_check.exists():\n",
    "    logger.info(f\"SUCCESS: File found at {absolute_path_check}\")\n",
    "    logger.info(f\"File size: {absolute_path_check.stat().st_size} bytes\")\n",
    "    # You can also try loading it\n",
    "    try:\n",
    "        data_check = np.load(absolute_path_check, allow_pickle=True)\n",
    "        logger.info(f\"Successfully loaded npz file. Keys found: {list(data_check.keys())}\")\n",
    "        logger.info(f\"Processed data shape from file: {data_check['processed_data'].shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Found file but FAILED to load/read npz file: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.error(f\"FAILURE: File NOT found at {absolute_path_check}\")\n",
    "    logger.info(\"Listing directory contents:\")\n",
    "    parent_dir = absolute_path_check.parent\n",
    "    if parent_dir.exists():\n",
    "         logger.info(f\"Contents of {parent_dir}: {list(parent_dir.iterdir())}\")\n",
    "    else:\n",
    "         logger.error(f\"Parent directory {parent_dir} does not exist.\")\n",
    "\n",
    "logger.info(\"--- Final check finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
