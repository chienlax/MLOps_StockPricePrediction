**Comprehensive Review Document: MLOps Stock Prediction Pipeline**

**Part 1: Summary of Achievements**

Throughout this session, we have successfully transformed a set of Python scripts for stock prediction into a robust, automated MLOps pipeline. Key achievements include:

1.  **Full Automation with Airflow:** Two interconnected Airflow DAGs (`daily_stock_operations_prod_dag` and `stock_model_retraining_prod_dag`) now orchestrate the entire lifecycle.
2.  **Database-Centric Architecture:** PostgreSQL serves as the central store for:
    *   Raw stock data (`raw_stock_data`).
    *   Versioned processed datasets (`processed_feature_data` keyed by `dataset_run_id`).
    *   Versioned scaled features and scalers (`scaled_feature_sets`, `scalers` keyed by `dataset_run_id`).
    *   Versioned optimized hyperparameters (`optimization_results` keyed by `dataset_run_id`).
    *   Daily predictions with model attribution (`latest_predictions` with `target_prediction_date` and `model_mlflow_run_id`).
    *   Historical model performance metrics (`model_performance_log`).
3.  **MLflow Integration for Model Lifecycle Management:**
    *   **Experiment Tracking:** All training runs log parameters (including the `dataset_run_id` for data lineage), metrics, and model artifacts.
    *   **Model Registry:** Trained models are registered, versioned, and managed using aliases (e.g., "Production") for deployment.
4.  **CI/CT (Continuous Training & Integration) Loop:**
    *   The daily operations DAG monitors production model performance.
    *   If performance degrades below configured thresholds, it automatically triggers the retraining DAG.
    *   The retraining DAG produces a new candidate model, evaluates it, and promotes it to "Production" if it's superior, thus closing the loop.
5.  **Parameterization and Modularity:** Python scripts have been refactored to accept command-line arguments (especially `--run_id` and `--mode`), making them reusable components within the Airflow tasks.
6.  **Version Control for Data and Models:**
    *   Dataset versions are tracked via the `dataset_run_id`.
    *   Model versions are managed by MLflow Model Registry.
7.  **Operational Monitoring:** A dedicated script and database table (`model_performance_log`) track the daily accuracy of the production model.
8.  **API-Ready Predictions:** The system generates daily prediction outputs (both in the database and as JSON files) that can be consumed by the existing FastAPI application.

This setup provides a solid foundation for reliable, reproducible, and continuously improving stock predictions.

---

**Part 2: Problem/Solution Log - The Refactoring Journey**

This section details the key challenges encountered and the changes made to address them:

1.  **Problem:** Initial scripts likely had hardcoded paths and lacked mechanisms for versioning data or being called parametrically by an orchestrator.
    *   **Solution:**
        *   Introduced command-line arguments to all core scripts (`make_dataset.py`, `build_features.py`, `optimize_hyperparams.py`, `train_model.py`), especially `--config` and `--run_id`.
        *   Shifted from file-based storage of intermediate artifacts (processed data, scalers, params) to PostgreSQL tables, versioned using a unique `dataset_run_id`.

2.  **Problem:** Need for distinct operational modes in data processing: daily incremental raw data updates vs. full reprocessing for training.
    *   **Solution:**
        *   Refactored `make_dataset.py` to support two modes via a `--mode` argument:
            *   `incremental_fetch`: Updates `raw_stock_data` with only new data.
            *   `full_process`: Fetches all raw data, performs full feature engineering, and saves to `processed_feature_data` with a new `dataset_run_id`.

3.  **Problem:** Passing the `dataset_run_id` generated by one Airflow task (data processing) to subsequent tasks (feature building, optimization, training).
    *   **Solution:**
        *   Modified `make_dataset.py` (in `full_process` mode) to print the generated `dataset_run_id` in a specific format (`RUN_ID:<id>`).
        *   Changed the Airflow task for data processing to a `PythonOperator` (`callable_run_make_dataset_full_process`) that executes the script via `subprocess`, captures the stdout, parses the `run_id`, and explicitly pushes it to XComs with a clear key (e.g., `new_dataset_run_id`).
        *   Downstream `BashOperator` tasks then use Jinja templating (`{{ ti.xcom_pull(...) }}`) to inject this `run_id` into their script calls.

4.  **Problem:** Initial MLflow Model Registry integration error: "No Production model found."
    *   **Cause:** The daily operations DAG was looking for a model in the "Production" stage/alias, but no model had been promoted yet.
    *   **Solution:**
        *   Recognized the need to run the retraining pipeline first to create and register a model.
        *   Manually promoted a model version to "Production" (or set the "Production" alias) in the MLflow UI for initial testing of the daily DAG.
        *   Ensured the full retraining DAG includes an automated evaluation and promotion step using aliases.

5.  **Problem:** `AttributeError: 'ModelVersion' object has no attribute 'stage'` when trying to construct a model URI using `.stage`.
    *   **Cause:** MLflow's deprecation of "stages" means the `.stage` attribute is unreliable or gone in newer versions of `ModelVersion` objects.
    *   **Solution:**
        *   Shifted from using stages to **aliases** for identifying the production model (e.g., an alias named "Production").
        *   Modified `callable_get_production_model_details_daily` (in the daily DAG) to use `client.get_model_version_by_alias()` to fetch the production model.
        *   This callable now pushes the base model name, version number, and the alias-based loading URI to XComs.
        *   Updated the promotion logic in the retraining DAG (`callable_evaluate_and_branch_promotion` and `callable_promote_model_to_production`) to use `client.set_registered_model_alias()`.

6.  **Problem:** `RESOURCE_DOES_NOT_EXIST: Registered Model with name=<name_with_alias> not found` in `predict_model.py`.
    *   **Cause:** `predict_model.py` was incorrectly trying to use a model URI containing an alias (e.g., `models:/MyModel@Production`) as the `name` parameter for `MlflowClient` methods like `get_latest_versions` or `get_model_version`, which expect the base model name.
    *   **Solution:**
        *   Modified `callable_get_production_model_details_daily` (in the daily DAG) to push the *base registered model name* and the specific *version number* (obtained via the alias) to XComs separately.
        *   Refactored `predict_model.py` to accept these separate base name and version number arguments. It now uses these to correctly call `client.get_model_version(name=base_name, version=number)` to fetch metadata, while still using the alias URI (`models:/MyModel@Production`) for `mlflow.pytorch.load_model()`.

7.  **Problem:** MLflow `load_model` hanging or failing with "500 error responses" during artifact download.
    *   **Cause:** The MLflow server (running in its Docker container) was unable to access or serve the model artifact files from its configured artifact store (`/mlruns` mapped to `airflow-mlflow-volume`). This is often due to file non-existence at the expected path or permission issues on the volume.
    *   **Solution (Debugging Steps Provided):**
        *   Inspect the `mlflow-server` container logs for specific errors.
        *   Exec into the `mlflow-server` container and verify the existence and permissions of the artifact files within its `/mlruns` directory structure.
        *   Ensure consistency in UIDs or group permissions between the process writing the artifacts (Airflow worker) and the process reading them (MLflow server).

8.  **Problem:** `monitor_performance.py` initially relying on local JSON files for predictions, which is not ideal for SSoT and can cause "file not found" if the data flow isn't perfect.
    *   **Cause:** Pragmatic initial choice due to the `latest_predictions` DB table not initially storing `target_prediction_date`.
    *   **Solution (Implemented):**
        *   Enhanced the `latest_predictions` DB table schema to include `target_prediction_date DATE NOT NULL` and made the primary key `(target_prediction_date, ticker)`.
        *   Modified `db_utils.save_prediction` to accept and store `target_prediction_date`.
        *   Updated `predict_model.py` to pass the `target_prediction_date` when saving predictions to the database.
        *   Refactored `monitor_performance.py` to query the enhanced `latest_predictions` table using `get_prediction_for_date_ticker()` instead of reading JSON files for the predictions to be evaluated.

9.  **Problem:** Ensuring Python scripts executed by `BashOperator` can find custom project modules.
    *   **Solution:** Consistently added `PYTHONPATH` to the `env` dictionary of `BashOperator` tasks, pointing to the project root and the `src` directory within the Airflow worker container (e.g., `env={'PYTHONPATH': "/opt/airflow:/opt/airflow/src"}`).

---

**Part 3: Final DAGs Workflow Deep Dive**

This revisits the detailed flow, emphasizing the connections.

**A. `daily_stock_operations_prod_dag`**

*   **Overall Purpose:** Runs daily to ingest new data, make predictions with the current best model, log these predictions, monitor the performance of past predictions, and trigger a model retraining if performance is subpar.

| Task ID                                 | Operator          | Script Executed (from `/opt/airflow/`)      | Purpose & Key Inputs                                                                                                                                                                 | Key Outputs & Side Effects                                                                                                                                                                                                                              | `params.yaml` Sections |
| :-------------------------------------- | :---------------- | :------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------- |
| `initialize_database_daily`             | `PythonOperator`  | (Callable `setup_database`)                 | `CONFIG_PATH` (for DB connection details).                                                                                                                                             | Ensures all necessary PostgreSQL tables (`raw_stock_data`, `latest_predictions`, `model_performance_log`, etc.) exist and have the correct schema.                                                                                                           | `database`             |
| `fetch_incremental_raw_data_daily`      | `BashOperator`    | `src/data/make_dataset.py`                  | Args: `--config CONFIG_PATH`, `--mode incremental_fetch`. Reads `raw_stock_data` (for last dates).                                                                                     | Fetches new stock data from Yahoo Finance. Updates/inserts into `raw_stock_data` table.                                                                                                                                                               | `database`, `data_loading` |
| `get_production_model_details_daily`    | `PythonOperator`  | (Callable `...get_prod_model_details...`)   | `CONFIG_PATH` (for MLflow settings). Queries MLflow Model Registry for model aliased "Production".                                                                                     | Pushes to XComs: `production_model_uri_for_loading`, `production_model_base_name`, `production_model_version_number`, `production_model_training_dataset_run_id` (the `dataset_run_id` the prod model was trained on).                                       | `mlflow`               |
| `prepare_daily_prediction_input`        | `BashOperator`    | `src/features/prepare_prediction_input.py`  | Args: `--config CONFIG_PATH`, `--production_model_training_run_id` (XCom), `--output_dir <temp_dir>`. Reads `scalers`, `processed_feature_data` (using XCom `dataset_run_id`). Reads `raw_stock_data`. | Generates a `.npy` file (e.g., `/tmp/.../input.npy`) with the scaled input sequence. Prints `OUTPUT_PATH:<filepath>` to stdout (captured as XCom `return_value`).                                                                                       | `database`, `feature_engineering` |
| `make_daily_prediction`                 | `BashOperator`    | `src/models/predict_model.py`               | Args: `--config CONFIG_PATH`, `--input_sequence_path` (XCom, parsed from stdout), `--production_model_uri_for_loading`, `--production_model_base_name`, `--production_model_version_number` (all from XCom). Reads `.npy` file. Loads model from MLflow. Reads `scalers` (using model's `dataset_run_id`). | Makes predictions. Saves to `latest_predictions` table (with `target_prediction_date` & model's MLflow run ID). Creates/updates `latest_predictions.json` & `historical/{date}.json` (for API).                                                           | `database`, `mlflow`, `output_paths` |
| `monitor_model_performance_daily_task`  | `BashOperator`    | `src/models/monitor_performance.py`         | Args: `--config CONFIG_PATH`. Reads `latest_predictions` table (for `prediction_date_to_evaluate`). Reads `raw_stock_data` (for actuals).                                           | Calculates metrics. Saves metrics to `model_performance_log` table. Prints `NEXT_TASK_ID:<branch_task_id>` to stdout (captured as XCom `return_value`).                                                                                                  | `database`, `data_loading`, `monitoring`, `airflow_dags` |
| `branch_based_on_performance`         | `BranchPythonOp`  | (Callable `...branch_on_monitoring...`)     | Pulls XCom `return_value` from `monitor_model_performance_daily_task`.                                                                                                               | Returns a Task ID string (`retraining_trigger_task_id` or `no_retraining_task_id`) to direct Airflow execution.                                                                                                                                     | `airflow_dags`         |
| (Task ID from `params.yaml`, e.g., `trigger_retraining_pipeline_task`) | `TriggerDagRunOp` | N/A                                         | `trigger_dag_id` (ID of retraining DAG from `params.yaml`).                                                                                                          | Initiates a run of the `stock_model_retraining_prod_dag`.                                                                                                                                                                                               | `airflow_dags`         |
| (Task ID from `params.yaml`, e.g., `no_retraining_needed_task`)       | `DummyOperator`   | N/A                                         | N/A                                                                                                                                                                    | Path taken if performance is acceptable. Ends this branch of the daily workflow.                                                                                                                                                                    | N/A                    |

**B. `stock_model_retraining_prod_dag`**

*   **Overall Purpose:** Triggered when model performance is poor or manually. It creates a new, potentially better model using all up-to-date data and, if successful, promotes it to be the new "Production" model.

| Task ID                                  | Operator          | Script Executed (from `/opt/airflow/`)                      | Purpose & Key Inputs                                                                                                                                                                           | Key Outputs & Side Effects                                                                                                                                                                                                                            | `params.yaml` Sections |
| :--------------------------------------- | :---------------- | :---------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------- |
| `initialize_database_for_retraining`     | `PythonOperator`  | (Callable `setup_database`)                                   | `CONFIG_PATH`.                                                                                                                                                                                 | Ensures PostgreSQL tables exist.                                                                                                                                                                                                                     | `database`             |
| `process_all_data_for_retraining_task` | `PythonOperator`  | `src/data/make_dataset.py` (`callable...`, mode=`full_process`) | Args: `--config CONFIG_PATH`, `--mode full_process`. Reads `raw_stock_data`.                                                                                                                   | Updates `raw_stock_data`. Creates a new row in `processed_feature_data` with a **new `dataset_run_id`**. Pushes this `new_dataset_run_id` to XComs.                                                                                           | `database`, `data_loading`, `feature_engineering` |
| `build_features_for_retraining`        | `BashOperator`    | `src/features/build_features.py`                              | Args: `--config CONFIG_PATH`, `--run_id` (XCom `new_dataset_run_id`). Reads `processed_feature_data` for this `run_id`.                                                                           | Creates rows in `scaled_feature_sets` and `scalers` tables, linked to the `new_dataset_run_id`.                                                                                                                                                   | `database`, `feature_engineering` |
| `optimize_hyperparams_for_retraining`  | `BashOperator`    | `src/models/optimize_hyperparams.py`                          | Args: `--config CONFIG_PATH`, `--run_id` (XCom `new_dataset_run_id`). Reads `scaled_feature_sets` and `scalers` for this `run_id`.                                                                   | Creates row in `optimization_results` table, linked to `new_dataset_run_id`.                                                                                                                                                                      | `database`, `optimization` |
| `train_candidate_model_task`           | `PythonOperator`  | `src.models.train_model.py` (`callable...`)                   | Args: `--config CONFIG_PATH`, `--run_id` (XCom `new_dataset_run_id`). Reads DB for data, scalers, params linked to `run_id`.                                                                         | Trains model. Logs to **new MLflow Run** (params incl. `new_dataset_run_id`, metrics, model artifact). Registers model in MLflow Registry. Pushes **candidate's MLflow Run ID** & its `new_dataset_run_id` to XComs.                     | `database`, `training`, `mlflow`, `output_paths` |
| `evaluate_and_branch_on_promotion_decision`| `BranchPythonOp`  | (Callable `...evaluate_and_branch...`)                        | Pulls XComs: `candidate_model_mlflow_run_id`, `candidate_training_dataset_run_id`. `CONFIG_PATH`. Queries MLflow for candidate & prod model metrics. (May read test data from DB for candidate). | Compares candidate vs. production. Returns Task ID string (`promotion_task_id` or `no_promotion_task_id`). Pushes `candidate_model_version_to_promote_run_id` to XCom if promoting.                                                           | `mlflow`, `model_promotion` |
| (Task ID from `params.yaml`, e.g., `promote_candidate_to_production_task`) | `PythonOperator`  | (Callable `...promote_model...`)                              | Pulls XCom: `candidate_model_version_to_promote_run_id`. `CONFIG_PATH`.                                                                                                      | Finds model version for the candidate's MLflow run. Sets "Production" alias in MLflow Model Registry to this version (archives old if applicable).                                                                                             | `mlflow`               |
| (Task ID from `params.yaml`, e.g., `do_not_promote_task`)          | `DummyOperator`   | N/A                                                         | N/A                                                                                                                                                                            | Path taken if candidate model is not promoted.                                                                                                                                                                                                        | N/A                    |

This concludes our comprehensive review. You've built a sophisticated MLOps pipeline!