# MLOps Stock Prediction Project: Docker Environment Setup Guide

**Goal:** To set up a consistent, reproducible development environment using Docker for running Airflow, MLflow, and associated Python dependencies for the Automated Stock Market Price Prediction project.

**Authors:** [Your Name/Team Name]
**Date:** [Current Date]

## 1. Prerequisites

Before starting, ensure the following are installed on your local machine:

*   **Git:** For version control ([https://git-scm.com/download/](https://git-scm.com/download/))
*   **Docker Desktop:** For running containers. Ensure it's installed, running, and preferably using the WSL 2 backend on Windows ([https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)).

## 2. Project Structure

We are using the following standard project structure (ensure this is cloned from the Git repository):

```
MLOps_StockPricePrediction/
│
├── airflow/
│   ├── dags/               # Airflow DAG Python files
│   ├── logs/               # Mounted logs folder
│   └── plugins/            # Mounted plugins folder
├── config/                 # Configuration files (e.g., params.yaml)
├── data/                   # Data files (Tracked by DVC, NOT Git)
│   ├── raw/
│   ├── processed/
│   ├── features/
│   └── predictions/
├── src/                    # Python source code (data processing, models, api)
├── templates/              # HTML templates (if building UI)
├── tests/                  # Automated tests
├── .dvc/                   # DVC metadata
├── .github/                # GitHub Actions workflows (optional CI/CD)
│
├── .dockerignore           # Files to ignore during Docker build context
├── .env                    # Local environment variables (!! NOT COMMITTED TO GIT !!)
├── .env.example            # Example environment variables (Committed to Git)
├── .gitignore              # Files ignored by Git
├── Dockerfile              # Defines the custom Airflow image
├── docker-compose.yml      # Defines and orchestrates Docker services
├── LICENSE                 # Project License
├── README.md               # Project documentation (this file!)
├── requirements.in         # Direct Python dependencies (human-managed)
└── requirements.txt        # Pinned Python dependencies (generated by pip-compile)
```

## 3. Core Configuration Files

These files define the Dockerized environment. Ensure they are present and correct in your cloned repository.

### a) `Dockerfile`

Defines the custom Airflow image, building upon an official Airflow image and adding our project's dependencies.

```dockerfile
# Dockerfile

# Use Python 3.11 and a compatible recent Airflow version (e.g., 2.8.1)
ARG AIRFLOW_VERSION=2.8.1
ARG PYTHON_VERSION=3.11
FROM apache/airflow:${AIRFLOW_VERSION}-python${PYTHON_VERSION}

# Set user to root temporarily if system packages needed (usually not required)
# USER root
# RUN apt-get update && apt-get install -y --no-install-recommends some-package && apt-get clean
# USER airflow

# Copy requirements first to leverage Docker cache
COPY requirements.txt /requirements.txt

# Install Python dependencies from requirements.txt
# Add pip cache purge before install to prevent stale cache issues
RUN pip cache purge && \
    pip install --no-cache-dir -r /requirements.txt

# Optional: Copy source code if needed inside image (we use mount for dev)
# COPY src/ /opt/airflow/src/
```

*   **Key Points:** Uses Python 3.11, Airflow 2.8.1. Installs dependencies from `requirements.txt`. Includes `pip cache purge` before install.

### b) `docker-compose.yml`

Defines the services (Airflow webserver, scheduler, database, MLflow), networks, and volumes.

```yaml
# docker-compose.yml
x-airflow-common: &airflow-common
  build: . # Use the custom image built from Dockerfile
  env_file:
    - .env
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor # Or LocalExecutor for simpler setup
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    # CORRECT Database connection variable name for Airflow 2.6+
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__MLFLOW__TRACKING_URI: ${MLFLOW_TRACKING_URI:-http://mlflow-server:5000}
  volumes:
    - ./airflow/dags:/opt/airflow/dags:rw
    - ./airflow/logs:/opt/airflow/logs:rw
    - ./airflow/plugins:/opt/airflow/plugins:rw
    - ./src:/opt/airflow/src:ro
    - ./data:/opt/airflow/data:rw
    - ./config:/opt/airflow/config:ro
  depends_on:
    postgres:
      condition: service_healthy
    mlflow-server:
      condition: service_started

services:
  postgres:
    image: postgres:13
    container_name: stockpred-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGSTRS_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - airflow-db-volume:/var/lib/postgresql/data

  mlflow-server:
    # Use a known valid image tag
    image: ghcr.io/mlflow/mlflow:v2.13.1 # Example valid tag
    container_name: stockpred-mlflow-server
    ports:
      - "5001:5000"
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:////mlruns/mlflow.db --default-artifact-root /mlruns/artifacts
    volumes:
      - airflow-mlflow-volume:/mlruns

  airflow-webserver:
    <<: *airflow-common
    container_name: stockpred-airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: stockpred-airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$$HOSTNAME\""]
      interval: 60s
      timeout: 30s
      retries: 5

volumes:
  airflow-db-volume:
  airflow-mlflow-volume:
```

*   **Key Points:** Defines `postgres`, `mlflow-server`, `airflow-webserver`, `airflow-scheduler`. Mounts local folders (`dags`, `src`, `data`, etc.) into containers. Uses `.env` file. Uses correct `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` variable. Uses valid MLflow image tag.

### c) `requirements.in`

Lists direct Python dependencies with flexible version constraints. Used by `pip-compile`.

```txt
# requirements.in (Example - Use your actual file)
# === Core & Data ===
pandas>=1.5.0,<2.2.0
numpy>=1.21.0,<1.27.0
python-dotenv>=1.0.0,<1.1.0

# === Data Source ===
yfinance>=0.2.10,<0.3.0

# === ML Models & Utils ===
scikit-learn>=1.1.0,<1.4.0
statsmodels>=0.13.0,<0.15.0
# Choose one:
torch>=2.2.0,<2.7.0
# tensorflow>=2.12.0,<2.16.0 # Ensure compatibility

# === MLOps Tools ===
mlflow>=2.13.0,<2.14.0 # Use latest compatible version
dvc>=2.50.0,<3.0.0

# === Visualization ===
plotly>=5.10.0,<5.20.0
matplotlib>=3.5.0,<3.9.0
seaborn>=0.12.0,<0.14.0

# === API / Dashboard ===
fastapi[all]>=0.90.0,<0.110.0

# === Airflow Specific Providers ===
apache-airflow-providers-postgres
apache-airflow-providers-docker
apache-airflow-providers-openlineage>=1.8.0 # Added explicit constraint

# === Notebook Support (Commented out for Docker build) ===
# ipykernel>=6.15.0,<6.27.0
# notebook>=6.5.0,<7.1.0
```

*   **Key Points:** List only direct requirements. Comment out local dev tools (`notebook`, `ipykernel`). Add explicit constraints for providers if compatibility issues arise (like `openlineage`). Use up-to-date versions (like for `mlflow`) to resolve conflicts.

### d) `.env` / `.env.example`

Used for local configuration and secrets.

*   **`.env.example` (Commit this to Git):**
    ```dotenv
    # Example .env file - Copy to .env and fill in values
    # Ensure UID/GID match your host user if needed for volume permissions (often 1000/0 on Win/Mac Docker Desktop)
    AIRFLOW_UID=1000
    AIRFLOW_GID=0

    # Internal Docker network URI for MLflow
    MLFLOW_TRACKING_URI=http://mlflow-server:5000

    # Add API Keys etc. here
    # ALPHA_VANTAGE_API_KEY=YOUR_KEY
    ```
*   **`.env` (Create locally, !! DO NOT COMMIT TO GIT !!):** Copy `.env.example` to `.env` and modify as needed. Ensure `.env` is listed in your `.gitignore` file.

## 4. Setup Workflow (For Team Members)

Follow these steps after cloning the repository:

1.  **Install Prerequisites:** Ensure Git and Docker Desktop are installed and running.
2.  **Navigate to Project:** Open your terminal in the project's root directory.
3.  **Create `.env` File:** Copy `.env.example` to `.env`. Review and adjust `AIRFLOW_UID`/`AIRFLOW_GID` if necessary (usually `1000`/`0` is fine for Docker Desktop). Add any required API keys.
4.  **Generate `requirements.txt` (Important!):** Run `pip-compile` *inside* a temporary Docker container to ensure Linux compatibility and avoid issues like the `pywin32` error. **Delete `requirements.txt` if it exists first.**
    ```bash
    # Use PowerShell version (Recommended)
    docker run --rm -v "${PWD}/requirements.in:/app/requirements.in:ro" -v "${PWD}:/app/output" apache/airflow:2.8.1-python3.11 bash -c "pip install pip-tools && cd /app && pip-compile requirements.in --output-file /app/output/requirements.txt"

    # Or use CMD version with full absolute path if needed
    # docker run --rm -v "C:\full\path\to\your\project\requirements.in:/app/requirements.in:ro" -v "C:\full\path\to\your\project:/app/output" apache/airflow:2.8.1-python3.11 bash -c "pip install pip-tools && cd /app && pip-compile requirements.in --output-file /app/output/requirements.txt"
    ```
    *(This step ensures the pinned dependencies are correct for the Linux environment inside Docker)*
5.  **First Time Airflow DB Setup (If needed):** The first person setting up might need to manually initialize the database and create the first user *after* starting services (or if the containers exit mentioning DB issues). If you encounter errors about DB initialization later:
    ```bash
    # Ensure services are down first: docker compose down -v
    # Then run these commands:
    docker compose run --rm airflow-scheduler airflow db init
    docker compose run --rm airflow-scheduler airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin # Use a secure password!
    ```
6.  **Build & Start Containers:**
    ```bash
    # Build the custom Airflow image (needed first time or after Dockerfile/requirements.txt changes)
    docker compose build

    # Start all services in detached mode
    docker compose up -d
    ```
7.  **Verify:**
    *   Wait 30-60 seconds for services to stabilize.
    *   Run `docker ps`. Check that `stockpred-postgres`, `stockpred-mlflow-server`, `stockpred-airflow-scheduler`, `stockpred-airflow-webserver` are all `Up`.
    *   Access Airflow UI: `http://localhost:8080` (Login: `admin`/`admin` or your password)
    *   Access MLflow UI: `http://localhost:5001`

## 5. Troubleshooting Common Issues Encountered

We faced several issues during setup. Here’s a summary:

1.  **Problem:** `pip-compile` fails locally due to version conflicts (e.g., PyTorch/TensorFlow constraints for Python 3.12 vs 3.11).
    *   **Cause:** Local Python version differs from Docker target; specified version ranges unavailable on PyPI for the local environment.
    *   **Solution:** Adjust version constraints in `requirements.in` to match ranges available for the *target* Python version (3.11) or use broader compatible ranges. Run `pip-compile` inside Docker (Step 4 above).

2.  **Problem:** `pip-compile` fails locally due to `protobuf` dependency conflict (`mlflow` needs `<5`, `opentelemetry` needs `>=5`).
    *   **Cause:** Different libraries requiring incompatible major versions of `protobuf`.
    *   **Solution:** Update the constraint for the library requiring the older version (usually `mlflow`) in `requirements.in` to a newer version known to be compatible with `protobuf>=5` (e.g., `mlflow>=2.13.0`). Rerun `pip-compile` inside Docker.

3.  **Problem:** `docker compose build` fails with `ERROR: No matching distribution found for pywin32==3xx`.
    *   **Cause:** `pip-compile` running on Windows added the Windows-only `pywin32` dependency (required by packages like `notebook` or `docker`) to `requirements.txt`. This package cannot be installed on Linux in the Docker container.
    *   **Solution:**
        *   Comment out/remove development tools (`notebook`, `ipykernel`) from `requirements.in`.
        *   **Crucially:** Generate `requirements.txt` by running `pip-compile` *inside* the Linux Docker container (see Setup Workflow Step 4) so platform-specific dependencies aren't included.

4.  **Problem:** `docker compose build` fails with `ERROR: THESE PACKAGES DO NOT MATCH THE HASHES...`.
    *   **Cause:** Package files on PyPI changed slightly (same version, different content/hash) between when `pip-compile` generated `requirements.txt` and when `docker build` ran `pip install`. Pip's internal build cache can also contribute.
    *   **Solution:**
        *   Add `pip cache purge && \` before `pip install` in the `Dockerfile`.
        *   Regenerate `requirements.txt` (using the inside-Docker method) *immediately* before running `docker compose build`.
        *   If persistent, build with `docker compose build --no-cache`.

5.  **Problem:** `docker compose up` fails, error resolving `ghcr.io/mlflow/mlflow:vX.Y.Z: not found`.
    *   **Cause:** The specified MLflow image tag doesn't exist on GitHub Container Registry.
    *   **Solution:** Update the `image:` tag for the `mlflow-server` service in `docker-compose.yml` to a known valid tag (e.g., `ghcr.io/mlflow/mlflow:v2.13.1`). Check [ghcr.io/mlflow/mlflow/tags](https://ghcr.io/mlflow/mlflow/tags) for available tags.

6.  **Problem:** Airflow containers (webserver/scheduler) exit after starting. Logs show provider incompatibility errors (e.g., `apache-airflow-providers-openlineage`).
    *   **Cause:** Dependency resolution picked a provider version incompatible with the base Airflow version's runtime checks.
    *   **Solution:** Add an explicit, compatible version constraint for the problematic provider (e.g., `apache-airflow-providers-openlineage>=1.8.0`) to `requirements.in`. Regenerate `requirements.txt` inside Docker, rebuild image (`docker compose build`), and restart (`docker compose up -d`).

7.  **Problem:** Airflow webserver exits. Logs show `ERROR: You need to initialize the database. Please run \`airflow db init\`.`
    *   **Cause:** Airflow metadata tables missing or unrecognized in the database. Often related to failed initialization or volume issues. Also potentially caused by incorrect DB connection string variable.
    *   **Solution:**
        *   Ensure `docker-compose.yml` uses `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` (not `AIRFLOW__CORE__...`).
        *   Stop and remove containers and volumes: `docker compose down -v`.
        *   Manually run DB initialization: `docker compose run --rm airflow-scheduler airflow db init`.
        *   Manually create user: `docker compose run --rm airflow-scheduler airflow users create ...`.
        *   Restart services: `docker compose up -d`.

## 6. Development Workflow

*   **Start Services:** `docker compose up -d`
*   **Edit Code:** Modify Python files in `src/` and DAG files in `airflow/dags/` locally. Changes are reflected inside the running containers due to volume mounts.
*   **Test/Run:** Trigger DAGs via the Airflow UI (`http://localhost:8080`).
*   **Monitor:** Check logs via `docker compose logs <service_name>` (e.g., `airflow-scheduler`). Check experiment results in MLflow UI (`http://localhost:5001`).
*   **Version Control:** Use Git frequently (`git add .`, `git commit`, `git push`, `git pull`).
*   **Data Versioning (DVC):** Set up DVC remote storage. Use `dvc add data/...`, `git commit`, `dvc push` to share data. Use `git pull` and `dvc pull` to retrieve data.

## 7. Important Notes

*   **NEVER commit the `.env` file to Git.** Use `.env.example` for templates.
*   Ensure `.gitignore` correctly ignores the `data/` directory contents (DVC handles data).
*   Regularly `git pull` to stay synced with teammates.
*   If `Dockerfile` or `requirements.txt` changes, rebuild the image using `docker compose build`.
